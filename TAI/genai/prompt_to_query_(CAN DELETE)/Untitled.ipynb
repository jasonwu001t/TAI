{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41be3839",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735db017",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec2851b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tables have been saved as Parquet files.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Table 1: Historical Usage Data\n",
    "historical_usage_data = pd.DataFrame({\n",
    "    'usage_id': range(1, 11),\n",
    "    'product_group': ['c6i', 'c5', 'c6g', 'c6i', 'c5', 'c6g', 'c6i', 'c5', 'c6g', 'c6i'],\n",
    "    'date': pd.date_range(start='2023-01-01', periods=10, freq='M'),\n",
    "    'cpu_usage': np.random.randint(500, 1500, size=10),\n",
    "    'memory_usage': np.random.randint(1000, 8000, size=10),\n",
    "    'storage_usage': np.random.randint(10000, 50000, size=10)\n",
    "})\n",
    "\n",
    "# Save historical usage data to Parquet\n",
    "historical_usage_data.to_parquet('historical_usage_data.parquet', index=False)\n",
    "\n",
    "# Table 2: Forecasting Results\n",
    "forecasting_results = pd.DataFrame({\n",
    "    'forecast_id': range(1, 11),\n",
    "    'product_group': ['c6i', 'c5', 'c6g', 'c6i', 'c5', 'c6g', 'c6i', 'c5', 'c6g', 'c6i'],\n",
    "    'scenario_id': np.random.randint(1, 4, size=10),\n",
    "    'forecast_date': pd.date_range(start='2024-01-01', periods=10, freq='M'),\n",
    "    'cpu_forecast': np.random.randint(1000, 2000, size=10),\n",
    "    'memory_forecast': np.random.randint(5000, 10000, size=10),\n",
    "    'storage_forecast': np.random.randint(20000, 80000, size=10)\n",
    "})\n",
    "\n",
    "# Save forecasting results to Parquet\n",
    "forecasting_results.to_parquet('forecasting_results.parquet', index=False)\n",
    "\n",
    "# Table 3: Scenario Snapshots\n",
    "scenario_snapshots = pd.DataFrame({\n",
    "    'scenario_id': range(1, 4),\n",
    "    'scenario_name': ['Base Case', 'High Demand', 'Low Demand'],\n",
    "    'snapshot_date': pd.to_datetime(['2024-01-01', '2024-01-15', '2024-01-30']),\n",
    "    'description': ['Normal growth forecast', 'Higher than expected growth', 'Lower than expected growth']\n",
    "})\n",
    "\n",
    "# Save scenario snapshots to Parquet\n",
    "scenario_snapshots.to_parquet('scenario_snapshots.parquet', index=False)\n",
    "\n",
    "# Table 4: Product Group CPU Mapping\n",
    "product_group_cpu_mapping = pd.DataFrame({\n",
    "    'product_group': ['c6i', 'c5', 'c6g'],\n",
    "    'cpu_type': ['Intel Ice Lake', 'Intel Cascade Lake', 'Graviton2'],\n",
    "    'generation': ['Ice Lake', 'Cascade Lake', 'Graviton2']\n",
    "})\n",
    "\n",
    "# Save product group CPU mapping to Parquet\n",
    "product_group_cpu_mapping.to_parquet('product_group_cpu_mapping.parquet', index=False)\n",
    "\n",
    "# Table 5: Product Group Details\n",
    "product_group_details = pd.DataFrame({\n",
    "    'product_group': ['c6i', 'c5', 'c6g'],\n",
    "    'vcpus': [64, 48, 64],\n",
    "    'memory_gb': [128, 96, 128],\n",
    "    'storage_gb': [2000, 1500, 2000],\n",
    "    'network_performance': ['Up to 25 Gbps', 'Up to 10 Gbps', 'Up to 25 Gbps']\n",
    "})\n",
    "\n",
    "# Save product group details to Parquet\n",
    "product_group_details.to_parquet('product_group_details.parquet', index=False)\n",
    "\n",
    "print(\"All tables have been saved as Parquet files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9e29b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5206b934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>product_group</th><th>cpu_type</th><th>generation</th></tr><tr><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;c6i&quot;</td><td>&quot;Intel Ice Lake&quot;</td><td>&quot;Ice Lake&quot;</td></tr><tr><td>&quot;c5&quot;</td><td>&quot;Intel Cascade Lake&quot;</td><td>&quot;Cascade Lake&quot;</td></tr><tr><td>&quot;c6g&quot;</td><td>&quot;Graviton2&quot;</td><td>&quot;Graviton2&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 3)\n",
       "┌───────────────┬────────────────────┬──────────────┐\n",
       "│ product_group ┆ cpu_type           ┆ generation   │\n",
       "│ ---           ┆ ---                ┆ ---          │\n",
       "│ str           ┆ str                ┆ str          │\n",
       "╞═══════════════╪════════════════════╪══════════════╡\n",
       "│ c6i           ┆ Intel Ice Lake     ┆ Ice Lake     │\n",
       "│ c5            ┆ Intel Cascade Lake ┆ Cascade Lake │\n",
       "│ c6g           ┆ Graviton2          ┆ Graviton2    │\n",
       "└───────────────┴────────────────────┴──────────────┘"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.read_parquet('product_group_cpu_mapping.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc3b94ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ./product_group_cpu_mapping.parquet as table 'product_group_cpu_mapping'\n",
      "Loaded ./historical_usage_data.parquet as table 'historical_usage_data'\n",
      "Loaded ./product_group_details.parquet as table 'product_group_details'\n",
      "Loaded ./forecasting_results.parquet as table 'forecasting_results'\n",
      "Loaded ./scenario_snapshots.parquet as table 'scenario_snapshots'\n",
      "shape: (3, 4)\n",
      "┌─────────────┬───────────────┬─────────────────────┬─────────────────────────────┐\n",
      "│ scenario_id ┆ scenario_name ┆ snapshot_date       ┆ description                 │\n",
      "│ ---         ┆ ---           ┆ ---                 ┆ ---                         │\n",
      "│ i64         ┆ str           ┆ datetime[ns]        ┆ str                         │\n",
      "╞═════════════╪═══════════════╪═════════════════════╪═════════════════════════════╡\n",
      "│ 1           ┆ Base Case     ┆ 2024-01-01 00:00:00 ┆ Normal growth forecast      │\n",
      "│ 2           ┆ High Demand   ┆ 2024-01-15 00:00:00 ┆ Higher than expected growth │\n",
      "│ 3           ┆ Low Demand    ┆ 2024-01-30 00:00:00 ┆ Lower than expected growth  │\n",
      "└─────────────┴───────────────┴─────────────────────┴─────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import os\n",
    "\n",
    "class PH:\n",
    "    def __init__(self, directory=\".\"):\n",
    "        self.dataframes = {}\n",
    "        self.load_parquet_files(directory)\n",
    "\n",
    "    def read_parquet(self, file_path, table_name):\n",
    "        \"\"\"\n",
    "        Reads a Parquet file into a Polars DataFrame and stores it in the handler with the given table_name.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            df = pl.read_parquet(file_path)\n",
    "            self.dataframes[table_name] = df\n",
    "            print(f\"Successfully read {file_path} into table '{table_name}'\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read {file_path}: {e}\")\n",
    "            \n",
    "    def load_parquet_files(self, directory):\n",
    "        \"\"\"\n",
    "        Load all Parquet files in the specified directory into Polars DataFrames.\n",
    "        \n",
    "        Args:\n",
    "        directory (str): The directory where the Parquet files are stored.\n",
    "        \"\"\"\n",
    "        for filename in os.listdir(directory):\n",
    "            if filename.endswith(\".parquet\"):\n",
    "                table_name = os.path.splitext(filename)[0]\n",
    "                file_path = os.path.join(directory, filename)\n",
    "                self.dataframes[table_name] = pl.read_parquet(file_path)\n",
    "                print(f\"Loaded {file_path} as table '{table_name}'\")\n",
    "    \n",
    "    \n",
    "    def query(self, sql_query): #self, table_name, sql_query\n",
    "        \"\"\"\n",
    "        Execute an SQL query using Polars SQL context.\n",
    "        \n",
    "        Args:\n",
    "        sql_query (str): The SQL query string.\n",
    "        \n",
    "        Returns:\n",
    "        Polars DataFrame: The result of the SQL query.\n",
    "        \"\"\"\n",
    "        context = pl.SQLContext(self.dataframes)\n",
    "        try:\n",
    "            result = context.execute(sql_query).collect()\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"Query execution error: {e}\")\n",
    "            return None\n",
    "#         \"\"\"\n",
    "#         Queries a stored DataFrame using Polars SQL and returns the result.\n",
    "#         \"\"\"\n",
    "#         if table_name in self.dataframes:\n",
    "#             try:\n",
    "#                 context = pl.SQLContext(self.dataframes)\n",
    "#                 result = context.execute(sql_query).collect()\n",
    "#                 return result\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Failed to execute query: {e}\")\n",
    "#                 return None\n",
    "#         else:\n",
    "#             print(f\"Table '{table_name}' not found.\")\n",
    "#             return None\n",
    "    \n",
    "    def get_table(self, table_name):\n",
    "        \"\"\"\n",
    "        Returns the Polars DataFrame for the given table_name.\n",
    "        \"\"\"\n",
    "        if table_name in self.dataframes:\n",
    "            return self.dataframes[table_name]\n",
    "        else:\n",
    "            print(f\"Table '{table_name}' not found.\")\n",
    "            return None\n",
    "\n",
    "# Example usage:\n",
    "# Initialize the SQL handler\n",
    "sql_handler = PH()\n",
    "\n",
    "# Example query\n",
    "# query = \"SELECT AVG(cpu_usage) FROM historical_usage_data WHERE product_group = 'c6i' AND date = '2023-01-31';\"\n",
    "query = 'SELECT * FROM scenario_snapshots'\n",
    "result = sql_handler.query(query)\n",
    "\n",
    "if result is not None:\n",
    "    print(result)\n",
    "\n",
    "# # Create the handler\n",
    "# handler = PolarHandler()\n",
    "\n",
    "# # Read Parquet files into tables\n",
    "# handler.read_parquet('historical_usage_data.parquet', 'historical_usage')\n",
    "# handler.read_parquet('forecasting_results.parquet', 'forecasting_results')\n",
    "# handler.read_parquet('scenario_snapshots.parquet', 'scenario_snapshots')\n",
    "# handler.read_parquet('product_group_cpu_mapping.parquet', 'product_group_cpu_mapping')\n",
    "# handler.read_parquet('product_group_details.parquet', 'product_group_details')\n",
    "\n",
    "# # Direct access example\n",
    "# historical_usage_df = handler.get_table('historical_usage')\n",
    "# print(historical_usage_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4a4a4111",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "polars.dataframe.frame.DataFrame"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sql_handler.get_table('scenario_snapshots'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0dd2e3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (3, 4)\n",
      "┌─────────────┬───────────────┬─────────────────────┬─────────────────────────────┐\n",
      "│ scenario_id ┆ scenario_name ┆ snapshot_date       ┆ description                 │\n",
      "│ ---         ┆ ---           ┆ ---                 ┆ ---                         │\n",
      "│ i64         ┆ str           ┆ datetime[ns]        ┆ str                         │\n",
      "╞═════════════╪═══════════════╪═════════════════════╪═════════════════════════════╡\n",
      "│ 1           ┆ Base Case     ┆ 2024-01-01 00:00:00 ┆ Normal growth forecast      │\n",
      "│ 2           ┆ High Demand   ┆ 2024-01-15 00:00:00 ┆ Higher than expected growth │\n",
      "│ 3           ┆ Low Demand    ┆ 2024-01-30 00:00:00 ┆ Lower than expected growth  │\n",
      "└─────────────┴───────────────┴─────────────────────┴─────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Query example\n",
    "query_result = handler.query('scenario_snapshots', \n",
    "                             \"SELECT * FROM scenario_snapshots\")\n",
    "print(query_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52831cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "TABLE_DETAILS = {\n",
    "    \"historical_usage\": \"This table stores historical usage data for different product groups, including CPU, memory, and storage usage.\",\n",
    "    \"forecasting_results\": \"This table contains forecasting results based on various scenarios, including future CPU, memory, and storage needs.\",\n",
    "    \"scenario_snapshots\": \"This table holds snapshots of different scenarios used in forecasting, with descriptions and snapshot dates.\",\n",
    "    \"product_group_cpu_mapping\": \"This table maps product groups to their respective CPU types and generations.\",\n",
    "    \"product_group_details\": \"This table provides detailed information about each product group, including vCPUs, memory, storage, and network performance.\",\n",
    "}\n",
    "\n",
    "\n",
    "SQL_TEMPLATE_STR = \"\"\"Given an input question, first create a syntactically correct SQL query to run, then look at the results of the query and return the answer.\n",
    "    You can order the results by a relevant column to return the most insightful data in the database.\\n\\n\n",
    "    Never query for all the columns from a specific table, only ask for a few relevant columns given the question.\\n\\n\n",
    "    Pay attention to use only the column names that you can see in the schema description. Be careful to not query for columns that do not exist.\n",
    "    Qualify column names with the table name when needed.\n",
    "\n",
    "    If a column name contains a space, always wrap the column name in double quotes.\n",
    "\n",
    "    You are required to use the following format, each taking one line:\\n\\nQuestion: Question here\\nSQLQuery: SQL Query to run\\n\n",
    "    SQLResult: Result of the SQLQuery\\nAnswer: Final answer here\\n\\nOnly use tables listed below.\\n{schema}\\n\\n\n",
    "    Do not under any circumstance use SELECT * in your query.\n",
    "    If the user is asking about hardware usage—interpret it as querying the historical usage data.\n",
    "\n",
    "    Here are some useful examples:\n",
    "    {few_shot_examples}\n",
    "\n",
    "    Question: {query_str}\\nSQLQuery: \"\"\"\n",
    "\n",
    "RESPONSE_TEMPLATE_STR = \"\"\"If the <SQL Response> below contains data, then given an input question, synthesize a response from the query results.\n",
    "    If the <SQL Response> is empty, then you should not synthesize a response and instead respond that no data was found for the question.\\n\n",
    "\n",
    "    \\nQuery: {query_str}\\nSQL: {sql_query}\\n<SQL Response>: {context_str}\\n</SQL Response>\\n\n",
    "\n",
    "    Do not make any mention of queries or databases in your response, instead you can say 'according to the latest data' .\\n\\n\n",
    "    Please make sure to mention any additional details from the context supporting your response.\n",
    "    If the final answer contains <dollar_sign>$</dollar_sign>, ADD '\\' ahead of each <dollar_sign>$</dollar_sign>.\n",
    "\n",
    "    Response: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9e342a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
